This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
agent.py
main.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="agent.py">
import ollama
import re

class mainLLM:
    def __init__(self):
        self.model = "gemma3:4b"
    
    def call_llm(self, prompt: str):
        try:
            response = ollama.generate(model=self.model, prompt=prompt)
            result = response['response']
            return result
        except Exception as e:
            return f"Error: {e}"
    
    def get_action_plan(self, user_input: str, available_tools: str):
        prompt = f"""
        you have access to those tools for web navigation and data extraction from the browser environment:
        {available_tools}
        User wants to: {user_input}
        Create a complete web navigation plan To get the required information from the web. Return ONLY JSON:
        {{
            "goal": "{user_input}",
            "steps": [
                {{  
                    "order": 1,
                    "action": "search_google_safely",
                    "query": "search query",
                    "description": "What to look for in the search results"
                }},
                {{
                    "order": 2,
                    "action": "click_result", 
                    "target": "which result to click",
                    "description": "navigate to specific site"
                }},
                {{
                    "order": 3,
                    "action": "extract_data",
                    "what": "what information to extract",
                    "description": "final data extraction"
                }}
                ... and so on up to 7 steps max.
            ]
        }}
        note:
        -The target in the click_result should be an element that should be available on the page after the search or an element that is likely to be present on the page.
        -make use of save_results action to store intermediate results if needed.
        -You should even go to new page and again search for something else if needed to achieve the goal.
        Keep it to min 3-7 steps max. Be specific about search queries and targets.
        """
        return self.call_llm(prompt)
    
    def extract_final_data(self, content: str, original_goal: str):
        """Final call to extract structured data"""
        prompt = f"""
        Original goal: {original_goal}
        Page content: {content[:1500]}
        Extract the requested information in clean JSON format.
        Return ONLY JSON as this structure:
        {{
            "results": [
                {{
                    # Example item structure - adapt as needed the below fields
                    "title": "item name",
                    "main_response": "price if asked, color if asked ,or anything if asked not just these fields you should have any fields relevant to the goal",
                    "description": "brief description", #should be there
                    "source": "website name" #should be there
                    ... if any more fields relevant to the goal
                }}
            ]
        }}
        """
        return self.call_llm(prompt)
</file>

<file path="main.py">
import asyncio
import requests
import json
from playwright.sync_api import sync_playwright
import time
import random
from agent import mainLLM
import re

class SimpleAgent:
    def __init__(self):
        self.playwright = sync_playwright().start()
        self.browser = self.playwright.chromium.launch( #browser instance modified to reduct bot like behavior
            headless=False,
            args=[
                '--disable-blink-features=AutomationControlled',
                '--disable-features=VizDisplayCompositor',
                '--disable-background-timer-throttling',
                '--disable-backgrounding-occluded-windows',
                '--disable-renderer-backgrounding'
            ]
        )
        user_agents = [
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        ]
        self.context = self.browser.new_context(
            viewport={"width": 1366, "height": 768},
            user_agent=random.choice(user_agents),
            extra_http_headers={
                "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
                "Accept-Language": "en-US,en;q=0.9",
                "Accept-Encoding": "gzip, deflate, br",
                "DNT": "1",
                "Connection": "keep-alive",
                "Upgrade-Insecure-Requests": "1",
            }
        )
        self.page = self.context.new_page()
        self.page.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', {
                get: () => undefined,
            })
        """)
        self.tools = {
            "search_google_safely": {
                "method": self.search_google_safely,
                "description": "Search Google for a query and return top results takes the target query as input string",
            },
            "click_result": {
                "method": self.smart_click,
                "description": "To click on a search result based on the context provided takes a target as input string",
            },
            "extract_data": {
                "method": self.extract_visible_content,
                "description": "Extract visible text content from the current page and filters out unnecessary content of the raw webpage"
            },
            "take_screenshot": {
                "method": self.take_screenshot,
                "description": "Take a screenshot of the current page for debugging purposes takes filename as input string"
            },
            "close_browser": {
                "method": self.close,
                "description": "Close the browser and cleanup resources"
            },
            "get_page_info": {
                "method": self.get_page_info,
                "description": "Get current page information such as URL, title, and a preview of the content"
            },"save_results":{
                "method": self.save_results,
                "description": "Append results to a local file for record-keeping of current context(i.e the last step web page context will be saved using extract data use this when you want to save the current context) and then using this at the last step to get final results"
            }
        }

    def save_results(self):
        try:
            with open("results_log.txt", "a") as f:
                f.write(f"URL: {self.page.url}\n")
                f.write(f"Title: {self.page.title()}\n")
                f.write(f"Content Preview: {self.extract_visible_content()[:500]}\n")
                f.write("="*50 + "\n")
            return "Results saved to results_log.txt"
        except Exception as e:
            return f"Error saving results: {e}"
    def human_like_delay(self, min_sec=1, max_sec=4):
        time.sleep(random.uniform(min_sec, max_sec))

    def human_like_mouse_movement(self):
        try:

            for _ in range(3):
                x = random.randint(100, 1200)
                y = random.randint(100, 600)
                self.page.mouse.move(x, y)
                time.sleep(0.2)
        except:
            pass
    
    def search_google_safely(self, query: str):
        try:
            print(f"üîç Searching for: {query}")
            self.page.goto("https://www.google.com", wait_until="networkidle")
            self.human_like_delay(1, 3)
            search_box = "textarea[name='q'], input[name='q']"
            self.page.click(search_box)
            self.human_like_delay(0.5, 1)
            for char in query:
                self.page.type(search_box, char, delay=random.randint(50, 150))
                if random.random() > 0.8:  # Random pauses
                    time.sleep(random.uniform(0.1, 0.3))
            self.page.press(search_box, "Enter")
            self.human_like_delay(3, 5)
            if "unusual traffic" in self.page.content().lower() or "detected unusual traffic" in self.page.content().lower():
                print("üö´ Blocked by Google. Trying alternative approach...")
                return self.use_alternative_search_engine(query)
            return self.get_page_info()
        except Exception as e:
            return {"error": str(e)}
    
    def use_alternative_search_engine(self, query: str):
        alternatives = [
            ("https://www.bing.com", "input[name='q']"),
            ("https://duckduckgo.com", "input[name='q']"),
            ("https://search.yahoo.com", "input[name='p']")
        ]
        for url, selector in alternatives:
            try:
                self.page.goto(url, wait_until="networkidle")
                self.page.fill(selector, query)
                self.page.press(selector, "Enter")
                return self.get_page_info()
            except:
                continue
        return {"error": "All search engines blocked"}
    
    def smart_click(self, target: str):
        strategies = [
            f"a:has-text('{target}')",
            f"h3:has-text('{target}')",
            f"div:has-text('{target}')",
            f"span:has-text('{target}')",
            f"button:has-text('{target}')",
            f"a:has-text(/:.*{target}.*/i)",
            f"h3:has-text(/:.*{target}.*/i)",
        ]
        
        for selector in strategies:
            try:
                if self.page.locator(selector).count() > 0:
                    self.human_like_mouse_movement()
                    self.page.click(selector)
                    self.human_like_delay(1, 2)
                    if self.page.url != self.page.context.pages[0].url:
                        return self.get_page_info()
            except:
                continue
        return "Click failed - element not found"

    def extract_visible_content(self):
        try:
            content = self.page.evaluate("""
                () => {
                    const elementsToRemove = document.querySelectorAll('script, style, nav, footer, header');
                    elementsToRemove.forEach(el => el.remove());
                    return document.body.innerText;
                }
            """)
            lines = [line.strip() for line in content.split('\n') if line.strip()]
            clean_content = '\n'.join(lines[:100])
            return clean_content[:2000]
        except Exception as e:
            return f"Extraction error: {e}"
    
    def get_page_info(self):
        try:
            return {
                "url": self.page.url,
                "title": self.page.title(),
                "content_preview": self.extract_visible_content()[:500]
            }
        except:
            return {"error": "Could not get page info"}
    
    def take_screenshot(self, filename: str):
        try:
            self.page.screenshot(path=f"screenshots/{filename}.png")
            return True
        except:
            return False
    
    def close(self):
        self.browser.close()
        self.playwright.stop()

    
    def get_tools_prompt(self):
        tools_text = ""
        for tool_name, tool_info in self.tools.items():
            tools_text += f"‚Ä¢ {tool_name}: {tool_info['description']}\n"
        return tools_text
    def parse_json(self, text: str):
        """Extract JSON from LLM response"""
        try:
            clean_text = re.sub(r'[^\x00-\x7F]+', '', text)
            json_match = re.search(r'\{.*\}', clean_text, re.DOTALL)
            if json_match:
                return json.loads(json_match.group())
            return {"error": "No JSON found in response"}
        except Exception as e:
            return {"error": f"JSON parsing failed: {e}"}
    

def main():
    agent = SimpleAgent()
    mainllm=mainLLM()
    try:
        while True:
            input_query=input("How can I assist you sir?")
            if input_query.lower() in ["exit", "quit", "q"]:
                print("Thank you for using me ::::")
                break  
            available_tools=agent.get_tools_prompt()
            action_plan_json=mainllm.get_action_plan(input_query,available_tools)
            action_plan=agent.parse_json(action_plan_json)
            print("Action Plan:", json.dumps(action_plan, indent=2))
            try:
                results = []
                ind=0
                
                for i, step in enumerate(action_plan.get("steps", [])):
                    print(f"\n{i+1}. ‚ö° Executing: {step.get('description', 'Unknown step')}")
                    
                    action = step.get("action", "")
                    if action in agent.tools:
                        method = agent.tools[action]["method"]
                        params = {k: v for k, v in step.items() if k not in ["order", "action", "description"]}
                        print(f"Executing Step {step['order']}: {action} with params {params}")
                        
                        if action =="extract_data":
                            result=method()
                            results.append({"index":ind+1,"result": result})
                            ind+=1
                        elif action =="save_results":
                            results.append({"index":ind+1,"result": results})
                            ind+=1
                        else:
                            result = method(**params) if params else method()
                        
                        agent.take_screenshot(f"step_{i+1}")
                        print(f"‚úÖ Step {i+1} completed")
                        
                        # Check for blocking pages
                        page_content = str(result).lower()
                        if "unusual traffic" in page_content or "blocked" in page_content or "captcha" in page_content:
                            print("\nüö´ We've been blocked! Please solve the CAPTCHA manually.")
                            print("Press Enter to continue after solving...")
                            input()
                    else:
                        print(f"‚ùå Unknown action: {action}")
                
                # Final extraction after all steps
                print("\nüìä Analyzing final results...")
                final_content = agent.extract_visible_content()

                if not final_content:
                    print("‚ùå No final content extracted")
                    final_results = {"error": "No content extracted"}
                else:
                    results_str = "\n".join([f"Step {r['index']} : {r['result']}" for r in results])
                    final_response = mainllm.extract_final_data( results_str, input_query)
                    final_results = agent.parse_json(final_response)
                
                print(f"\nüéØ Final Results:")
                print(json.dumps(final_results, indent=2))
                
            except Exception as e:
                print(f"‚ùå Error during execution: {e}")
    except KeyboardInterrupt:
        print("Exiting...")
    finally:    
        agent.close()
if __name__ == "__main__":
    main()
</file>

</files>
